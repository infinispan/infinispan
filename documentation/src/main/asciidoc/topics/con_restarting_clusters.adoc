[id='cluster-restarts_{context}']
= Shutdown and restart of {brandname} clusters
Prevent data loss and ensure consistency of your cluster by properly shutting down and restarting nodes.

[discrete]
== Cluster shutdown
{brandname} recommends using the `shutdown cluster` command to stop all nodes in a cluster while saving cluster state and persisting all data in the cache.
You can use the `shutdown cluster` command also for clusters with a single node.

When you bring {brandname} clusters back online, all nodes and caches in the cluster will be unavailable until all nodes rejoin.
To prevent inconsistencies or data loss, {brandname} restricts access to the data stored in the cluster and modifications of the cluster state until the cluster is fully operational again.
Additionally, {brandname} disables cluster rebalancing and prevents local cache stores purging on startup.

During the cluster recovery process, the coordinator node logs messages for each new node joining, indicating which nodes are available and which are still missing.
Other nodes in the {brandname} cluster have the view from the time they join. You can monitor availability of caches using the {brandname} Console or REST API.

However, in cases where waiting for all nodes is not necessary nor desired, it is possible to set a cache available with the current topology.
This approach is possible through the CLI, see below, or the REST API.

[IMPORTANT]
====
Manually installing a topology can lead to data loss, only perform this operation if the initial topology cannot be recreated.
====

[discrete]
== Server shutdown
After using the `shutdown server` command to bring nodes down, the first node to come back online will be available immediately without waiting for other members.
The remaining nodes join the cluster immediately, triggering state transfer but loading the local persistence first, which might lead to stale entries.
Local cache stores configured to purge on startup will be emptied when the server starts.
Local cache stores marked as `purge=false` will be available after a server restarts but might contain stale entries.

If you shutdown clustered nodes with the [command]`shutdown server` command, you must restart each server in reverse order to avoid potential issues related to data loss and stale entries in the cache. +
For example, if you shutdown `server1` and then shutdown `server2`, you should first start `server2` and then start `server1`.
However, restarting clustered nodes in reverse order does not completely prevent data loss and stale entries.

[discrete]
== Stop container

The `shutdown` command requires all shutdown nodes to be restarted after the graceful shutdown.
This mechanism is recommended for maintenance operations, since it can incur downtime.
The command can be utilized to perform an upgrade, for example.

Additionally, {brandname} offers the option of stopping the caches in a cache container.
The command `leave` stops all caches in the cache container the CLI is connected to.
This command will immediately stop all caches in the container without waiting for pending operations.
This is useful in cases where data loss is acceptable, and for performing operations like in-place rolling upgrades to avoid downtime.
You can use the `leave` command to stop the caches and keep the server running while performing an upgrade.
Data loss may occur in a distributed cache when more than num-owners nodes leave in sequence, without waiting for the state transfer to complete.
In such a case, the `ISPN000312` message is logged as a warning, for example:
[source,log]
----
WARN  [o.i.CLUSTER] [Context=cache-name]ISPN000312: Lost data because of graceful leaver node-4
----

The `leave` command also has an option for timeout, for example `leave --timeout=30s`, where the operation waits up to a given time for caches to complete any state transfer operation.
This operation reduces the likelihood of data loss in distributed caches when stopping nodes in the cluster by waiting for the state transfer to complete.
This is the recommended approach if you are performing an in-place rolling upgrade and want to keep your data.

[WARNING]
====
The `leave` command with a timeout reduces the likelihood of losing data in distributed caches.
However, data loss can still happen if multiple nodes execute the `leave` command simultaneously, even with the timeout option.
====


[role="_additional-resources"]
.Additional resources
* link:{cli_docs}#leave1[leave CLI command]
