[[sid-68355040]]

==  Infinispan Server Modules

[[sid-68355040_InfinispanServerModules-Introduction]]


=== Introduction

Traditionally, clients have interacted with Infinispan in a peer-to-peer (p2p) fashion where Infinispan and the client code that accessed it lived within the same VM. When Infinispan is queried in this way, it is considered to be accessed in an embedded fashion, as shown in the screenshot below

 
.TODO InformalFigure image title empty
image::[]

 

[[sid-68355040_InfinispanServerModules-ClientServeroverPeertoPeer]]


==== Client-Server over Peer-to-Peer

 However, there are situations when accessing Infinispan in a client-server mode might make more sense than accessing it via p2p. For example, when trying to _access Infinispan from a non-JVM environment_ . Since Infinispan is written in Java, if someone had a $$C++$$ application that wanted to access it, it couldn't just do it in a p2p way. On the other hand, client-server would be perfectly suited here assuming that a language neutral protocol was used and the corresponding client and server implementations were available. 

 
.TODO InformalFigure image title empty
image::[]

 

 In other situations, Infinispan users want to have an _elastic application tier_ where you start/stop business processing servers very regularly. Now, if users deployed Infinispan configured with distribution or state transfer, startup time could be greatly influenced by the shuffling around of data that happens in these situations. So in the following diagram, assuming Infinispan was deployed in p2p mode, the app in the second server could not access Infinispan until state transfer had completed. 

 
.TODO InformalFigure image title empty
image::[]

 

This effectively means that bringing up new application-tier servers is impacted by things like state transfer because applications cannot access Infinispan until these processes have finished and if the state being shifted around is large, this could take some time. This is undesirable in an elastic environment where you want quick application-tier server turnaround and predictable startup times. Problems like this can be solved by accessing Infinispan in a client-server mode because starting a new application-tier server is just a matter of starting a lightweight client that can connect to the backing data grid server. No need for rehashing or state transfer to occur and as a result server startup times can be more predictable which is very important for modern cloud-based deployments where elasticity in your application tier is important.

 
.TODO InformalFigure image title empty
image::[]

 

 Other times, it's common to find multiple applications needing access to data storage. In this cases, you could in theory deploy an Infinispan instance per each of those applications but this could be wasteful and difficult to maintain. Thing about databases here, you don't deploy a database alongside each of your applications, do you? So, alternatively you could deploy Infinispan in client-server mode keeping a pool of Infinispan data grid nodes acting as a _shared storage tier for your applications_ . 

 
.TODO InformalFigure image title empty
image::[]

 

Deploying Infinispan in this way also allows you to manage each tier independently, for example, you can upgrade you application or app server without bringing down your Infinispan data grid nodes.

[[sid-68355040_InfinispanServerModules-PeertoPeeroverClientServer]]


==== to-Peer over Client-Server

Before talking about individual Infinispan server modules, it's worth mentioning that in spite of all the benefits, client-server Infinispan still has disadvantages over p2p. Firstly, p2p deployments are simpler than client-server ones because in p2p, all peers are equals to each other and hence this simplifies deployment. So, if this is the first time you're using Infinispan, p2p is likely to be easier for you to get going compared to client-server.

Client-server Infinispan requests are likely to take longer compared to p2p requests, due to the serialization and network cost in remote calls. So, this is an important factor to take in account when designing your application. For example, with replicated Infinispan caches, it might be more performant to have lightweight HTTP clients connecting to a server side application that accesses Infinispan in p2p mode, rather than having more heavyweight client side apps talking to Infinispan in client-server mode, particularly if data size handled is rather large. With distributed caches, the difference might not be so big because even in p2p deployments, you're not guaranteed to have all data available locally.

Environments where application tier elasticity is not so important, or where server side applications access state-transfer-disabled, replicated Infinispan cache instances are amongst scenarios where Infinispan p2p deployments can be more suited than client-server ones.

[[sid-68355040_InfinispanServerModules-ServerModules]]


=== Server Modules

So, now that it's clear when it makes sense to deploy Infinispan in client-server mode, what are available solutions? All Infinispan server modules are based on the same pattern where the server backend creates an embedded Infinispan instance and if you start multiple backends, they can form a cluster and share/distribute state if configured to do so. The server types below primarily differ in the type of listener endpoint used to handle incoming connections. Here's a brief look at each of them:

[[sid-68355040_InfinispanServerModules-RESTServerModule]]


==== REST Server Module


*  link:$$http://community.jboss.org/docs/14095$$[This module] , which is distributed as a WAR file, can be deployed in any servlet container to allow Infinispan to be accessed via link:$$https://docs.jboss.org/author/pages/viewpage.action?pageId=3737132$$[a RESTful HTTP interface] . 


* To connect to it, you can use any HTTP client out there and there're tons of different client implementations available out there for pretty much any language or system.


* This module is particularly recommended for those environments where HTTP port is the only access method allowed between clients and servers.


*  Clients wanting to load balance or failover between different Infinispan REST servers can do so using any standard HTTP load balancer such as link:$$http://www.jboss.org/mod_cluster$$[mod_cluster] . It's worth noting though these load balancers maintain a static view of the servers in the backend and if a new one was to be added, it would require manual update of the load balancer. 

[[sid-68355040_InfinispanServerModules-MemcachedServerModule]]


==== Memcached Server Module


*  link:$$https://docs.jboss.org/author/pages/viewpage.action?pageId=3737037$$[This module] is an implementation of the link:$$http://github.com/memcached/memcached/blob/master/doc/protocol.txt$$[Memcached text protocol] backed by Infinispan. 


*  To connect to it, you can use any of the link:$$http://code.google.com/p/memcached/wiki/Clients$$[existing Memcached clients] which are pretty diverse. 


* As opposed to Memcached servers, Infinispan based Memcached servers can actually be clustered and hence they can replicate or distribute data using consistent hash algorithms around the cluster. So, this module is particularly of interest to those users that want to provide failover capabilities to the data stored in Memcached servers.


* In terms of load balancing and failover, there're a few clients that can load balance or failover given a static list of server addresses (perl's Cache::Memcached for example) but any server addition or removal would require manual intervention.

[[sid-68355040_InfinispanServerModules-HotRodServerModule]]


==== Hot Rod Server Module


*  link:$$https://docs.jboss.org/author/pages/viewpage.action?pageId=3737146$$[This module] is an implementation of the link:$$https://docs.jboss.org/author/pages/viewpage.action?pageId=9470083$$[Hot Rod binary protocol] backed by Infinispan which allows clients to do dynamic load balancing and failover and smart routing. 


*  So far, a single link:$$https://docs.jboss.org/author/pages/viewpage.action?pageId=3737142$$[Java client] , which is the reference implementation, has been fully developed. A beta version of the Python client is link:$$http://infinispan.blogspot.com/2011/02/infinispan-python-client-beta-for-hot.html$$[available] , and a link:$$https://github.com/noelo/hotrod-jruby$$[JRuby] and link:$$https://github.com/torquebox/infinispan-ruby-client$$[pure Ruby] clients are in the making. 


* If you're clients are running Java, this should be your defacto server module choice because it allows for dynamic load balancing and failover. This means that Hot Rod clients can dynamically detect changes in the topology of Hot Rod servers as long as these are clustered, so when new nodes join or leave, clients update their Hot Rod server topology view. On top of that, when Hot Rod servers are configured with distribution, clients can detect where a particular key resides and so they can route requests smartly.


* Load balancing and failover is dynamically provided by Hot Rod client implementations using information provided by the server.

[[sid-68355040_InfinispanServerModules-WebSocketServerModule]]


==== WebSocket Server Module


*  link:$$https://docs.jboss.org/author/pages/viewpage.action?pageId=3737036$$[This module] enables Infinispan to be exposed over a link:$$http://en.wikipedia.org/wiki/WebSockets$$[Websocket] interface via a Javascript API. 


* This module is very specifically designed for Javascript clients and so that is the only client implementation available.


* This module is particularly suited for developers wanting to enable access to Infinispan instances from their Javascript codebase.


* Since websockets work on the same HTTP port, any HTTP load balancer would do to load balance and failover.

[[sid-68355040_InfinispanServerModules-ServerComparisonSummary]]


=== Server Comparison Summary

Here's a table comparing and summarizing the capabilities of distinct server modules:

 
.TODO InformalFigure image title empty
image::[]

 

